# BPFd: Running BCC tools remotely across systems and architectures

## Introduction

[BCC tools](https://github.com/iovisor/bcc/blob/master/README.md) are a suite of kernel tracing tools that allow systems engineers to efficiently and safely get a deep understanding into the inner workings of a Linux system. Because they can't crash the kernel, they are safer than kernel modules and also can be used in production. [Brendan Gregg has written](http://www.brendangregg.com/ebpf.html) several nice tools, and given talks showing the full power of eBPF based tools.

In the Android kernel team, we work mostly on ARM64 systems, since most Android devices are on this architecture. BCC tools support on ARM64 systems was broken for years. Recently this got [fixed](https://github.com/iovisor/bcc/issues/1202) and one could run the tools, but..

In order for BCC tools to work at all, they need kernel sources. This is because most tools need to register callbacks on the ever-changing kernel API in order to get their data. Such callbacks are registered using the [kprobe](https://lwn.net/Articles/132196/) infrastructure. When a BCC tool is run, BCC switches its current directory into the kernel source directory before compilation starts, and compiles the C program. The C program is free to include kernel headers for kprobes to work.

Even if one were not to use `kprobes`, BCC also implicity adds a common helpers.h include whenever an eBPF C program is being compiled, found in `src/cc/export/helpers.h` in the BCC sources. This helpers header uses `LINUX_VERSION_CODE` to create a "version" section in the compiled output. `LINUX_VERSION_CODE` is available only in the specific kernel's sources being target and is used during eBPF program loading to make sure the BPF program is being loaded into the correct kenrel version. As you can see, kernel sources quickly become mandatory for building eBPF programs.

In some sense this build process is similar to how external kernel modules are built. Kernel sources are large in size and often can take up a large amount of space on the system being debugged. They can also get out of sync, which may make the tools misbehave.

The other issue is Clang and LLVM libraries need to be available on the target being traced. This is because the tools compile the needed BPF bytecode which are then loaded into the kernel. These libraries take up a lot space. It seems overkill that you need a full-blown compiler infrastructure on a system when the BPF code can be compiled elsewhere and maybe even compiled just once. Further, these libraries need to be cross-compiled to run on the architecture you're tracing. That's possible, but why would anyone want to do that if they didn't need to? Cross-compiling compiler toolchains can be tedious and stressful.


## BPFd: A daemon for running eBPF BCC tools across systems
[Sources for BPFd can be downloaded here](https://github.com/joelagnel/bpfd).

Instead of loading up all the tools, compiler infrastructure and kernel sources onto the targets being traced and running BCC that way, I decided to write a proxy program named BPFd that receives commands and performs them on behalf of whoever is requesting them. All the heavily lifting (compilation, parsing of user input, parsing of the hash maps, presentation of results etc) is done by BCC tools on the host machine, with BPFd being the interface to the kernel, running on the target. BPFd encapsulates all the needs of BCC and performs them - this includes loading a BPF program, creating, deleting and looking up maps, attaching a eBPF program to a kprobe, polling for new data that the eBPF program may have written into a perf buffer, etc. If its woken up because the perf buffer contains new data, it'll inform BCC tools on the host about it, or it can return map data whenever rewuested, which may contain information updated by the target eBPF program.

### Simple design
The design of BPFd is quite simple, it expects commands on `stdin` (standard input) and provides the results over `stdout` (standard output). Every command a single line always, no matter how big the command is. This allows easy testing using `cat`, since one could simply `cat` a file with commands, and check if BPFd's `stdout` contain the expected results. Results from a command, however can be multiple lines.

For example, following is a command to BPFd for creating a BPF map:
```
BPF_CREATE_MAP 1 count 8 40 10240 0
```
And the result from BPFd is:
```
bpf_create_map: ret=3
```
The command tells BPFd to create a map named `count` with map type 1, with a key size of 8 bytes and a value size of 40, maximum of 10240 entries and no special flags. In return, BPFd created a map which is identified by file descriptor 3.

With the simple standard input/output design, it's possible to write wrappers around BPFd to handle more advanced communication such as USB or Networking. In the Android kernel team, we are planning communicatie these commands over [Android Debug Bridge](https://developer.android.com/studio/command-line/adb.html) which works over either USB or TCP/IP.

### Changes to BCC tools
[Branch with latest changes to BCC tools](https://github.com/joelagnel/bcc/tree/bcc-bpfd)

A new `remotes` module has been added to BCC tools with an abstraction that different remote mechanisms should adhere to. This keeps code duplication to a minimum. Currently an `adb` remote and a `process` remote have been added.  The `adb` remote is for communication with the target device over USB or TCP/IP using the [Android Debug Bridge](https://developer.android.com/studio/command-line/adb.html). The `process` remote is for local testing. With this remote, BPFd is forked on the same machine running BCC and communicates with it over `stdin` and `stdout`.

BCC tools modifications involve the following changes:
- Adding support for remote communication with BPFd such as over the network.
- Changes to BCC to send commands to the remote system.
- Making the kernel build more configurable so that kernel source path and target architecture information can be passed through environment variables.
- Changes to tools to accomodate the differences between remote and local modes.
- Some bug fixes.

### Installation procedure: Try it out for yourself!
If you want to just see examples of the results of running these tools, [you could jump](#running-filetop) straight to the tools section.
This is just an example of a typical installation procedure, it should be fairly straight forward to get these steps working for other remotes or architectures. For this example, we'll refer to the machine where you do all your development and have your kernel sources available as the `development machine` and the machine you're tracing as the `target`.

#### Install BCC tools on your development host
Follow the [steps here](https://github.com/iovisor/bcc/blob/master/INSTALL.md) to get BCC tools installed on your development machine, except for the following changes:
- Build BCC [using this branch](https://github.com/joelagnel/bcc/tree/bcc-bpfd)
- Read the section "Build LLVM and Clang development libs" in that document. Unfortunately, BPF inline-assembler support in LLVM is fairly recent, so you may need to build your own LLVM till then. Once LLVM is built, add `<path-to-llvm-source>/build/bin/` to your `$PATH` and then build BCC.

#### Build/Install BPFd on your target machine
[Clone and build the BPFd sources](https://github.com/joelagnel/bpfd). This really depends on the target distro, typically you can store it anywere `$PATH` on the target can find it, in Android we store it in `/data/` for now. In the future we'd want to store it in /system/bin/ once its an official Android package.

#### Prepare your kernel sources
- Make sure the kernel sources are available on your development machine somewhere, and that the kernel build has completed atleast once in the kernel source directory.

#### Setup environment variables and run tools
The following environment variables need to be setup:
- `ARCH` should point to the architecture of the `target` such as `x86` or `arm64`.
- `BCC_KERNEL_SOURCE` should point to the kernel source directory.
- `BCC_REMOTE` should point to the remote mechanism such as `adb`.

Two example environment variable settings are provided which can be sourced. Here's one for [adb interface with an arm64 Android target](https://github.com/joelagnel/bcc/blob/bcc-bpfd/arm64-adb.rc) and another one for a [local x86 target with a process remote](https://github.com/joelagnel/bcc/blob/bcc-bpfd/x86-local.rc).

### BPF Demos: examples of different BCC tools running on Android

#### Running filetop
`filetop` is a BCC tool which shows you all read/write I/O operations with a similar experience to the `top` tool. It refreshes every few seconds, giving you a live view of these operations.
Goto your bcc directory and set the env variables needed. For Android running on Hikey960, I run:
```
joel@ubuntu:~/bcc# source arm64-adb.rc
```
which basically sets the following environment variables:
```
  export ARCH=arm64
  export BCC_KERNEL_SOURCE=/home/joel/sdb/hikey-kernel/
  export BCC_REMOTE=adb
```
Next I start `filetop`:
```
joel@ubuntu:~/bcc# ./tools/filetop.py 5
```
This tells the tool to monitor file I/O every 5 seconds.

While `filetop` is running, I start the stock email app in Android and the output looks like:
```
  Tracing... Output every 5 secs. Hit Ctrl-C to end
  13:29:25 loadavg: 0.33 0.23 0.15 2/446 2931
 
  TID    COMM             READS  WRITES R_Kb    W_Kb    T FILE
  3787   Binder:2985_8    44     0      140     0       R profile.db
  3792   m.android.email  89     0      130     0       R Email.apk
  3813   AsyncTask #3     29     0      48      0       R EmailProvider.db
  3808   SharedPreferenc  1      0      16      0       R AndroidMail.Main.xml
  3811   SharedPreferenc  1      0      16      0       R UnifiedEmail.xml
  3792   m.android.email  2      0      16      0       R deviceName
  3815   SharedPreferenc  1      0      16      0       R MailAppProvider.xml
  3813   AsyncTask #3     8      0      12      0       R EmailProviderBody.db
  3809   AsyncTask #1     8      0      12      0       R suggestions.db
  2434   WifiService      4      0      4       0       R iface_stat_fmt
  2434   WifiService      4      0      4       0       R iface_stat_fmt
  3792   m.android.email  66     0      2       0       R framework-res.apk
```
Notice the Email.apk being read by Android to load the email application, and then various other reads happening related to the email app. Finally, WifiService continously reads iface_state_fmt to get network statistics for Android accounting.

#### Running biosnoop
Biosnoop is another great tool shows you block level I/O operations (bio) happening on the system along with the latency and size of the operation. Following is a sample output of running `tools/biosnoop.py` while doing random things in the Android system.
```
  TIME(s)        COMM           PID    DISK    T  SECTOR    BYTES   LAT(ms)
  0.000000000    jbd2/sdd13-8   2135   sdd     W  37414248  28672      1.90
  0.001563000    jbd2/sdd13-8   2135   sdd     W  37414304  4096       0.43
  0.003715000    jbd2/sdd13-8   2135   sdd     R  20648736  4096       1.94
  5.119298000    kworker/u16:1  3848   sdd     W  11968512  8192       1.72
  5.119421000    kworker/u16:1  3848   sdd     W  20357128  4096       1.80
  5.119443000    kworker/u16:1  3848   sdd     W  24551552  4096       1.82
  5.119462000    kworker/u16:1  3848   sdd     W  24551680  4096       1.82
  5.119478000    kworker/u16:1  3848   sdd     W  24617256  4096       1.83
  5.448831000    SettingsProvid 2415   sdd     W  20648752  8192       1.70
```
#### Running hardirq
This tool measures the total time taken by different hardirqs in the systems. Excessive time spent in hardirq can result in poor real-time performance of the system.

    joel@ubuntu:~/bcc# ./tools/hardirqs.py

Output:
```
  Tracing hard irq event time... Hit Ctrl-C to end.
  HARDIRQ                    TOTAL_usecs
  wl18xx                             232
  dw-mci                            1066
  e82c0000.mali                     8514
  kirin                             9977
  timer                            22384
```

#### Running biotop
Run biotop while launching the android Gallery app and doing random stuff:
```
joel@ubuntu:~/bcc# ./tools/biotop.py
```
Output:
```
PID    COMM             D MAJ MIN DISK       I/O  Kbytes  AVGms
4524   droid.gallery3d  R 8   48  ?           33    1744   0.51
2135   jbd2/sdd13-8     W 8   48  ?           15     356   0.32
4313   kworker/u16:4    W 8   48  ?           26     232   1.61
4529   Jit thread pool  R 8   48  ?            4     184   0.27
2135   jbd2/sdd13-8     R 8   48  ?            7      68   2.19
2459   LazyTaskWriterT  W 8   48  ?            3      12   1.77
2457   TaskSnapshotPer  W 8   48  ?            3      12   1.72
4541   GLThread 152     R 8   48  ?            1       4   1.81
4545   Error dump: sys  W 8   48  ?            1       4   1.70
```

### Open issues as of this writing
While most issues have been fixed, a few remain. Please check the [issue tracker](https://github.com/joelagnel/bpfd/issues) and contribute patches or help by testing.

### Conclusion and word on similiarity to SystemTap:
Building code for instrumentation on a different machine is beneficial and BPFd makes this possible. Tools like
[SystemTap](https://sourceware.org/systemtap/) have a similar feature. With SystemTap, one can build a kernel module
containing the instrumentation/probing code  using the SystemTap scripting language. There's a `stap` command for that,
and then the kernel module is copied over to the target and run using the `staprun` command. However, this is quite
unsafe since kernel modules can crash the kernel. Furthermore, the stap scripting language is quite limited in
comparison to the eBPF programs written in C. BCC tools are the future of such deep tracing endeavours. After all,
eBPF programs are verified before they're run and are guaranteed to be safe when loaded into the kernel, unlike
kernel modules.
